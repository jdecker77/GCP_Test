{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import coco\n",
    "# import utils\n",
    "# import model as modellib\n",
    "# import visualize\n",
    "# from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "# ROOT_DIR = os.getcwd()\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "\n",
    "# Import Mask RCNN\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "    \n",
    "    \n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco_humanpose.h5\")\n",
    "\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "# if not os.path.exists(COCO_MODEL_PATH):\n",
    "#     utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "# COCO_DIR = \"/home/xiaowu/Documents/Mask_RCNN_Humanpose-master/images\"  # TODO: enter you own value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'coco' has no attribute 'CocoConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e6f572929d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInferenceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mGPU_COUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mIMAGES_PER_GPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mUSE_MINI_MASK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mIMAGE_MIN_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'coco' has no attribute 'CocoConfig'"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2\n",
    "    USE_MINI_MASK = True\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", \n",
    "                          config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = COCO_MODEL_PATH\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True,exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "assert config.NAME == \"coco\"\n",
    "# Training dataset\n",
    "#load person keypoints dataset\n",
    "train_dataset_keypoints = coco.CocoDataset(task_type=\"person_keypoints\")\n",
    "train_dataset_keypoints.load_coco(COCO_DIR, \"train\")\n",
    "train_dataset_keypoints.prepare() \n",
    "print(\"Train Keypoints Image Count: {}\".format(len(train_dataset_keypoints.image_ids)))\n",
    "print(\"Train Keypoints Class Count: {}\".format(train_dataset_keypoints.num_classes))\n",
    "for i, info in enumerate(train_dataset_keypoints.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\n",
    "image_id = random.choice(train_dataset_keypoints.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask, gt_keypoint =\\\n",
    "    modellib.load_image_gt_keypoints(train_dataset_keypoints, config, \n",
    "                           image_id, augment=False,use_mini_mask=config.USE_MINI_MASK)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "log(\"gt_keypoint\", gt_keypoint)\n",
    "\n",
    "visualize.display_keypoints(original_image,gt_bbox,gt_keypoint,gt_class_id,train_dataset_keypoints.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Inspect the flipping augument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(train_dataset_keypoints.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask, gt_keypoint =\\\n",
    "    modellib.load_image_gt_keypoints(train_dataset_keypoints, config, \n",
    "                           image_id, augment=False,use_mini_mask=config.USE_MINI_MASK)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "log(\"gt_keypoint\", gt_keypoint)\n",
    "\n",
    "visualize.display_keypoints(original_image,gt_bbox,gt_keypoint,gt_class_id,train_dataset_keypoints.class_names)\n",
    "if(config.USE_MINI_MASK):\n",
    "    gt_mask = utils.expand_mask(gt_bbox,gt_mask,original_image.shape)\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            train_dataset_keypoints.class_names,)\n",
    "\n",
    "original_image_flip, image_meta_flip, gt_class_id_flip, gt_bbox_flip, gt_mask_flip, gt_keypoint_flip =\\\n",
    "    modellib.load_image_gt_keypoints(train_dataset_keypoints, config, \n",
    "                           image_id, augment=True,use_mini_mask=config.USE_MINI_MASK)\n",
    "\n",
    "\n",
    "visualize.display_keypoints(original_image_flip,gt_bbox_flip,gt_keypoint_flip,gt_class_id_flip,train_dataset_keypoints.class_names)\n",
    "if(config.USE_MINI_MASK):\n",
    "    gt_mask_flip = utils.expand_mask(gt_bbox_flip,gt_mask_flip,original_image_flip.shape)\n",
    "visualize.display_instances(original_image_flip, gt_bbox_flip, gt_mask_flip, gt_class_id_flip, \n",
    "                            train_dataset_keypoints.class_names,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inspect the DetectionKeypointTargetLayer\n",
    "This layers generated the ground truth keypoint labels and their values are between[0, 56*56).\n",
    "Here we run a sub graph to generated the ground truth target and visulize them to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rois, target_class_ids, target_bbox, target_keypoint, target_keypoint_weight, target_mask\n",
    "train_keypoint_generator = modellib.data_generator_keypoint(train_dataset_keypoints, config, shuffle=True,\n",
    "                                         batch_size=config.BATCH_SIZE)\n",
    "\n",
    "#python 3.* use nexy(generator)\n",
    "inputs,_ = next(train_keypoint_generator)\n",
    "batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox, batch_gt_class_ids, \\\n",
    "            batch_gt_boxes, batch_gt_keypoints,batch_gt_masks  = inputs\n",
    "log(\"batch_images\",batch_images)\n",
    "log(\"batch_image_meta\",batch_image_meta)\n",
    "log(\"batch_rpn_match\",batch_rpn_match)\n",
    "log(\"batch_rpn_bbox\",batch_rpn_bbox)\n",
    "log(\"batch_gt_class_ids\",batch_gt_class_ids)\n",
    "log(\"batch_gt_boxes\",batch_gt_boxes)\n",
    "log(\"batch_gt_keypoints\",batch_gt_keypoints)\n",
    "log(\"batch_gt_masks\",batch_gt_masks)\n",
    "\n",
    "rpn = model.run_graph(inputs, [\n",
    "    (\"rois\", model.keras_model.get_layer(\"proposal_targets\").output[0]),\n",
    "    (\"target_class_ids\", model.keras_model.get_layer(\"proposal_targets\").output[1]),\n",
    "    (\"target_bbox\", model.keras_model.get_layer(\"proposal_targets\").output[2]),\n",
    "    (\"target_keypoint_lables\", model.keras_model.get_layer(\"proposal_targets\").output[3]),\n",
    "    (\"target_keypoint_weights\", model.keras_model.get_layer(\"proposal_targets\").output[4]),\n",
    "    (\"target_mask\", model.keras_model.get_layer(\"proposal_targets\").output[5]),\n",
    "])\n",
    "\n",
    "roi = rpn[\"rois\"]\n",
    "target_class_ids = rpn[\"target_class_ids\"]\n",
    "target_bbox = rpn[\"target_bbox\"]\n",
    "target_keypoint_label = rpn[\"target_keypoint_lables\"]\n",
    "target_keypoint_weight = rpn[\"target_keypoint_weights\"]\n",
    "target_mask = rpn[\"target_mask\"]\n",
    "# gt_keypoints = rpn[\"gt_keypoints\"]\n",
    "keypoint_scales = [config.IMAGE_SHAPE[1],config.IMAGE_SHAPE[0],1]\n",
    "# gt_keypoints = keypoint_scales*gt_keypoints\n",
    "log(\"real\")\n",
    "for i in range(config.BATCH_SIZE):\n",
    "    batch_orignal_image = modellib.unmold_image(batch_images[i],config)\n",
    "    visualize.display_image_keypoint_mask(batch_orignal_image,roi[i],target_keypoint_label[i],target_keypoint_weight[i],\n",
    "                                          target_class_ids[i],train_dataset_keypoints.class_names,config=config,iskeypointlabel= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inspect some layers in build_fpn_keypoint_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = model.run_graph(inputs, [\n",
    "    (\"mrcnn_keypoint_mask_upsample_1\", model.keras_model.get_layer(\"mrcnn_keypoint_mask_upsample_1\").output),\n",
    "    (\"mrcnn_keypoint_mask_transpose\", model.keras_model.get_layer(\"mrcnn_keypoint_mask_transpose\").output),\n",
    "    (\"mrcnn_keypoint_mask_reshape\", model.keras_model.get_layer(\"mrcnn_keypoint_mask_reshape\").output),\n",
    "])\n",
    "plot.imshow(rpn[\"mrcnn_keypoint_mask_reshape\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Inspect the inner value of keypoint_mrcnn_mask_loss_graph\n",
    "    Before you run this cell, you must uncomment the \"test_mrcnn_mask_loss_graph\" in the model.py(Line 2726-2727) and add it in the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = model.run_graph(inputs, [\n",
    "    (\"pred_keypoint\", model.keras_model.get_layer(\"pred_keypoint\").output),\n",
    "    (\"target_keypoint\", model.keras_model.get_layer(\"target_keypoint\").output),\n",
    "    (\"target_class_ids_reshape\", model.keras_model.get_layer(\"target_class_ids_reshape\").output),\n",
    "    \n",
    "    (\"pred_keypoint_reshape\", model.keras_model.get_layer(\"pred_keypoint_reshape\").output),\n",
    "    (\"target_keypoint_reshape\", model.keras_model.get_layer(\"target_keypoint_reshape\").output),\n",
    "    (\"positive_pred_keypoint_masks\", model.keras_model.get_layer(\"positive_pred_keypoint_masks\").output),\n",
    "    (\"positive_target_keypoints\", model.keras_model.get_layer(\"positive_target_keypoints\").output),\n",
    "    (\"soft_loss\", model.keras_model.get_layer(\"soft_loss\").output),\n",
    "    (\"positive_loss\", model.keras_model.get_layer(\"positive_loss\").output),\n",
    "    (\"num_valid\", model.keras_model.get_layer(\"num_valid\").output),\n",
    "    (\"keypoint_loss\", model.keras_model.get_layer(\"keypoint_loss\").output),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

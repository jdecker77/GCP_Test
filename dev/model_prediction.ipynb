{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN for Pantograph Pose Estimation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been imported\n",
      "Using Root dir: /home/jupyter/GCP_Test\n",
      "Using Model dir: /home/jupyter/GCP_Test/models\n",
      "Using Data dir: /home/jupyter/GCP_Test/datasets/pantograph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Add root to path \n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "\n",
    "# Import Mask RCNN\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "from mrcnn.model import utils\n",
    "\n",
    "# Import pantogrograph class\n",
    "from dev import pantograph\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models\")\n",
    "\n",
    "# Set path to root of images. \n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"datasets/pantograph\")\n",
    "\n",
    "print(\"Using Root dir:\",ROOT_DIR)\n",
    "print(\"Using Model dir:\",MODEL_DIR)\n",
    "print(\"Using Data dir:\",DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Write json to file\n",
    "def WriteJSON(obj,filename):\n",
    "    try:\n",
    "        with open(filename, 'w') as outfile:\n",
    "#             obj_json = json.dumps(obj, sort_keys=True, indent=4,default=str)\n",
    "            obj_json = json.dumps(obj, cls=NumpyArrayEncoder)\n",
    "            outfile.write(obj_json)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('File not written.')\n",
    "\n",
    "# Read and return json object from file. If none, return empty object.\n",
    "def ReadJSON(filename):\n",
    "    try: \n",
    "        with open(filename) as f:\n",
    "            obj = json.loads(f.read())\n",
    "    except Exception as e: \n",
    "        obj = [] \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyArrayEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NumpyArrayEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "Run one of the code blocks below to import and load the configurations to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations Superlee:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_SHAPES                [[256 256]\n",
      " [128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        50\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0.5\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "KEYPOINT_MASK_POOL_SIZE        7\n",
      "KEYPOINT_MASK_SHAPE            [56, 56]\n",
      "KEYPOINT_THRESHOLD             0.005\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.002\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               128\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           pantograph\n",
      "NUM_CLASSES                    4\n",
      "NUM_KEYPOINTS                  6\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              2\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    150\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           100\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               100\n",
      "WEIGHT_DECAY                   0.0001\n",
      "WEIGHT_LOSS                    True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(pantograph.PantographConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on. Use CPU to leave GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\" # 'inference' or 'training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Skeleton: (5, 2)\n",
      "Keypoint names: (6,)\n",
      "Image Count: 24\n",
      "Class Count: 4\n",
      "  0. BG                                                \n",
      "  1. front_bar                                         \n",
      "  2. middle_bar                                        \n",
      "  3. rear_bar                                          \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = pantograph.PantographDataset()\n",
    "dataset.load_pantograph(DATA_DIR, \"val\")#test\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:385: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:409: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:980: The name tf.rint is deprecated. Please use tf.math.rint instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:980: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:989: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:991: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/GCP_Test/mrcnn/model.py:1007: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# Load model using selected device\n",
    "with tf.device(DEVICE):\n",
    "    \n",
    "    # Recreate the model in inference mode\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                              config=config,\n",
    "                              model_dir=MODEL_DIR)\n",
    "\n",
    "    # Set local path to trained weights file\n",
    "    LOG_DIR = os.path.join(MODEL_DIR, \"pantograph20200412T2157\")\n",
    "    MODEL_PATH = os.path.join(LOG_DIR, \"mask_rcnn_pantograph_0010.h5\")\n",
    "\n",
    "\n",
    "    # Load trained weights\n",
    "    model.load_weights(MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load sample image\n",
    "'''\n",
    "\n",
    "# Set random image_id\n",
    "image_id = random.choice(dataset.image_ids)\n",
    "\n",
    "# Set specific image_id\n",
    "image_id = 1\n",
    "\n",
    "print(\"image_id \", image_id, dataset.image_reference(image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Display Ground Truth Test Image\n",
    "'''\n",
    "\n",
    "image, image_meta, class_ids, bbox, masks, keypoints =\\\n",
    "    modellib.load_image_gt_keypoints(dataset, config, \n",
    "                           image_id, augment=False,use_mini_mask=False) # Set to False to preview original annotation\n",
    "\n",
    "# How to ensure alignment/where to get classnames, colors, etc??\n",
    "class_names = [dataset.class_names[1:][i-1] for i in class_ids]\n",
    "# print(len(class_names),len(dataset.class_names))\n",
    " \n",
    "if masks.shape[0] < image.shape[0]:\n",
    "    masks = utils.expand_mask(bbox, masks, image.shape)\n",
    "    \n",
    "visualize.display_instances(image, bbox, masks, class_ids,dataset.class_names,title=\"Predictions\")\n",
    "visualize.DrawAnnotations(image,class_ids,class_names,bbox=bbox,masks=masks,keypoints=keypoints,skeleton=dataset.skeleton,figsize=[16,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run Detection\n",
    "'''\n",
    "\n",
    "# Reload to be sure no changes\n",
    "info = dataset.image_info[image_id]\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "                                       dataset.image_reference(image_id)))\n",
    "\n",
    "# Run detection and get results\n",
    "results = model.detect_keypoint([image], verbose=1)\n",
    "r = results[0] # for one image\n",
    "\n",
    "# Get class names\n",
    "class_names = [dataset.class_names[1:][i-1] for i in class_ids]\n",
    "\n",
    "# Expand masks if needed\n",
    "if masks.shape[0] < image.shape[0]:\n",
    "    masks = utils.expand_mask(r['rois'], r['masks'], image.shape)\n",
    "else:\n",
    "    masks = r['masks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_instances(image, r['rois'], masks, r['class_ids'], \n",
    "                            dataset.class_names, r['scores'], \n",
    "                            title=\"Predictions\")\n",
    "\n",
    "# Draw vis\n",
    "visualize.DrawAnnotations(image, \n",
    "                          r['class_ids'],\n",
    "                          class_names=class_names,\n",
    "                          bbox=r['rois'],\n",
    "                          masks=masks,\n",
    "                          keypoints=r['keypoints'],\n",
    "                          skeleton=dataset.skeleton,\n",
    "                          scores=r['scores'],\n",
    "                          figsize=[16,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,cat_id in enumerate(r['class_ids']):\n",
    "    hit = False\n",
    "    if cat_id == dataset.class_ids[1:][i]:\n",
    "        hit = True\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run detection on multiple images. Results saved to JSON.\n",
    "'''\n",
    "\n",
    "def PredictMultiple(image_ids,RESULTS_FILE = '../datasets/pantograph/val/pred_region_data.json'):\n",
    "    \n",
    "    # Container for results\n",
    "    for image_id in image_ids:\n",
    "        \n",
    "        print(\"Running detection on:\", image_id, dataset.image_reference(image_id))\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Load image \n",
    "            image, image_meta, class_ids, bbox, masks, keypoints =\\\n",
    "            modellib.load_image_gt_keypoints(dataset, config, \n",
    "                                   image_id, augment=False,use_mini_mask=True)\n",
    "\n",
    "            # Save each result\n",
    "            results = model.detect_keypoint([image], verbose=0)\n",
    "            r = results[0] # for one image\n",
    "\n",
    "            # open pred json\n",
    "            obj = ReadJSON(RESULTS_FILE)\n",
    "\n",
    "            # Find image_id in images\n",
    "            ids = [i['id'] for i in obj['images']]\n",
    "            if image_id in ids:\n",
    "                print('Updating image information')\n",
    "                # Find image_id in annotations\n",
    "                obj['images'][image_id] = {\n",
    "                    'file_name':dataset.image_reference(image_id).split(\"/\")[-1],\n",
    "                    'height':image.shape[1],\n",
    "                    'id':image_id,\n",
    "                    'num_annotations':len(r['class_ids'].tolist()),\n",
    "                    'path':dataset.image_reference(image_id),\n",
    "                    'width':image.shape[0]\n",
    "                }\n",
    "                WriteJSON(obj,RESULTS_FILE)\n",
    "\n",
    "                for anno in obj['annotations']:\n",
    "                    obj = ReadJSON(RESULTS_FILE)\n",
    "                    for i,cat_id in enumerate(r['class_ids']):\n",
    "                        if image_id == anno['image_id'] and cat_id == anno['id']:\n",
    "\n",
    "                            # Determine if classification is successful\n",
    "                            hit = False\n",
    "                            if cat_id == dataset.class_ids[1:][i]:\n",
    "                                hit = True\n",
    "                            obj['annotations'][cat_id] = {\n",
    "                                'area':5463.6864,\n",
    "                                'category_id':cat_id,\n",
    "                                'id':cat_id,\n",
    "                                'image_id':image_id,\n",
    "                                'iscrowd':0,\n",
    "                                'num_keypoints':6,\n",
    "                                'hit':hit\n",
    "                            }\n",
    "                            WriteJSON(obj,RESULTS_FILE)\n",
    "\n",
    "            else:\n",
    "                print('Writing new image information')\n",
    "                img_info = {\n",
    "                    'file_name':dataset.image_reference(image_id).split(\"/\")[-1],\n",
    "                    'height':image.shape[1],\n",
    "                    'id':image_id,\n",
    "                    'num_annotations':len(r['class_ids'].tolist()),\n",
    "                    'path':dataset.image_reference(image_id),\n",
    "                    'width':image.shape[0],\n",
    "                }\n",
    "                obj['images'].append(img_info)\n",
    "                WriteJSON(obj,RESULTS_FILE)\n",
    "\n",
    "                for i in range(len(r['class_ids'].tolist())):\n",
    "                    hit = False\n",
    "                    if r['class_ids'].tolist()[i] == dataset.class_ids[1:][i]:\n",
    "                        hit = True\n",
    "    #             for cat_id in r['class_ids']:\n",
    "                    obj = ReadJSON(RESULTS_FILE)\n",
    "                    anno_info = {\n",
    "                        'area':5463.6864,\n",
    "                        'category_id':r['class_ids'].tolist()[i],\n",
    "                        'id':len(obj['annotations']),\n",
    "                        'image_id':image_id,\n",
    "                        'iscrowd':0,\n",
    "                        'num_keypoints':6,\n",
    "                        'bbox':r['rois'][i],\n",
    "                        'segmentation':r['masks'][i],\n",
    "                        'keypoints':r['keypoints'][i],\n",
    "                        'hit':hit\n",
    "                    }\n",
    "                    obj['annotations'].append(anno_info)\n",
    "                    WriteJSON(obj,RESULTS_FILE)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detection on: 0 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_220500.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 1 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_242996.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 2 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_37354.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 3 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_215562.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 4 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_130516.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 5 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_96772.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 6 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_237838.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 7 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_104182.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 8 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_185741.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 9 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_236870.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 10 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_48036.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 11 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_276816.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 12 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_170796.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 13 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_43716.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 14 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_106207.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 15 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_81516.jpg\n",
      "input type is not supported.\n",
      "Running detection on: 16 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_124097.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 17 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_135399.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 18 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_248419.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 19 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_213642.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 20 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_253661.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 21 /home/jupyter/GCP_Test/datasets/pantograph/val/padd_to_swin_frame_61936.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 22 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_59283.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n",
      "Running detection on: 23 /home/jupyter/GCP_Test/datasets/pantograph/val/swin_to_padd_frame_105019.jpg\n",
      "Molding Inputs\n",
      "(1, 1024, 1024, 3)\n",
      "Starting Detection\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "Writing new image information\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Call to run detection on multiple images\n",
    "'''\n",
    "\n",
    "image_ids = dataset.image_ids.tolist()[:]\n",
    "image_ids\n",
    "PredictMultiple(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
